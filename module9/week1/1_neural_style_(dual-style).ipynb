{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyZxAhvd6i85"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkxOtv6w6i86"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S15oJOFG6i87"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvnMQ2BS6i87"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrjwG_Ta6i87"
   },
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmI6sEcm6i87"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn4d8mqo6i88"
   },
   "outputs": [],
   "source": [
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = img_transforms(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "style_img1 = image_loader(\"style_img.jpg\")\n",
    "style_img2 = image_loader(\"style_img2.jpg\")\n",
    "content_img = image_loader(\"content_img.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2eN-LC16i88"
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "imshow(style_img1, title='Style Image1')\n",
    "\n",
    "plt.figure()\n",
    "imshow(style_img2, title='Style Image2')\n",
    "\n",
    "plt.figure()\n",
    "imshow(content_img, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u9b-jom6i88"
   },
   "source": [
    "## 3. Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_mxXpUQ6i89"
   },
   "source": [
    "### 3.1 Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_vTXTmw6i89"
   },
   "outputs": [],
   "source": [
    "content_weight = 1\n",
    "ContentLoss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyyYHCUq6i89"
   },
   "source": [
    "### 3.2 Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WFkS14J6i89"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    a, b, c, d = tensor.size()\n",
    "    tensor = tensor.view(a * b, c * d)\n",
    "    G = torch.mm(tensor, tensor.t())\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "style_weight = 1e6\n",
    "StyleLoss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZovf7JT6i89"
   },
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3291,
     "status": "ok",
     "timestamp": 1740532580570,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "gIZ-08Bz6i89",
    "outputId": "1644163a-1ee8-4bc4-97c8-a52eb9ab7b9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "VGG19_pretrained = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "VGG19_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740532580579,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "dS9ktoyaWo_g",
    "outputId": "1962ddef-b238-4aa1-de5b-6889fea52864"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-74401423e966>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
      "<ipython-input-11-74401423e966>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "normalization = Normalization().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4bX3Xw_6i89"
   },
   "outputs": [],
   "source": [
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2',\n",
    "                'conv_3', 'conv_4',\n",
    "                'conv_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EFoHggz6i89"
   },
   "outputs": [],
   "source": [
    "def get_features(pretrained_model, image):\n",
    "    layers = {\n",
    "        '0': 'conv_1',\n",
    "        '5': 'conv_2',\n",
    "        '10': 'conv_3',\n",
    "        '19': 'conv_4',\n",
    "        '28': 'conv_5'\n",
    "    }\n",
    "    features = {}\n",
    "    x = image\n",
    "    x = normalization(x)\n",
    "    for name, pretrained_layer in pretrained_model._modules.items():\n",
    "        x = pretrained_layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eNRHOnNNYas"
   },
   "outputs": [],
   "source": [
    "def get_dual_style(style_features1, style_features2, style_layers):\n",
    "    final_style_features = {}\n",
    "    for layer in style_layers:\n",
    "        sf1 = style_features1[layer]\n",
    "        sf2 = style_features2[layer]\n",
    "        ##############################YOUR CODE HERE############################\n",
    "        # 1. Calculate the size of the first portion of sf1 to be used.\n",
    "        #    This is a quarter of the number of channels (dimension 1).\n",
    "        # 2. Concatenate the first portion of sf1 with the second portion of sf2\n",
    "        # along the channel dimension.\n",
    "        #    - The first portion of sf1 should contain the first 'sf1_size' channels.\n",
    "        #    - The second portion of sf2 should contain the channels starting\n",
    "        #      from 'sf1_size' to the end.\n",
    "        ########################################################################\n",
    "    return final_style_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfH9P49i6i89"
   },
   "outputs": [],
   "source": [
    "content_features = get_features(VGG19_pretrained, content_img)\n",
    "style_features1 = get_features(VGG19_pretrained, style_img1)\n",
    "style_features2 = get_features(VGG19_pretrained, style_img2)\n",
    "final_style_features = get_dual_style(style_features1, style_features2, style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VgFr8cm6i89"
   },
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYFF__he6i89"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "target_img = content_img.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([target_img], lr=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nl1TMBv6NYat"
   },
   "outputs": [],
   "source": [
    "def style_tranfer_(model, optimizer, target_img,\n",
    "                   content_features, style_features,\n",
    "                   style_layers, content_weight, style_weight):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        target_img.clamp_(0, 1)\n",
    "    target_features = get_features(model, target_img)\n",
    "\n",
    "    content_loss = ContentLoss(content_features['conv_4'],\n",
    "                               target_features['conv_4'])\n",
    "\n",
    "    style_loss = 0\n",
    "    for layer in style_layers:\n",
    "        target_gram = gram_matrix(target_features[layer])\n",
    "        style_gram = gram_matrix(style_features[layer])\n",
    "        style_loss += StyleLoss(style_gram, target_gram)\n",
    "\n",
    "    total_loss = content_loss*content_weight + style_loss*style_weight\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    return total_loss, content_loss, style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43892,
     "status": "ok",
     "timestamp": 1740532624946,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "6UllrF3V6i89",
    "outputId": "42880b6d-8656-4d1c-df36-0f939d27c152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500] Total loss: 100.543045 -                 Content loss: 23.083927 - Style loss: 0.000077\n",
      "Epoch [200/500] Total loss: 95.259491 -                 Content loss: 23.274467 - Style loss: 0.000072\n",
      "Epoch [300/500] Total loss: 93.806984 -                 Content loss: 22.873281 - Style loss: 0.000071\n",
      "Epoch [400/500] Total loss: 92.336594 -                 Content loss: 22.623348 - Style loss: 0.000070\n",
      "Epoch [500/500] Total loss: 90.739052 -                 Content loss: 22.616415 - Style loss: 0.000068\n"
     ]
    }
   ],
   "source": [
    "STEPS = 500\n",
    "\n",
    "for step in range(STEPS):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        target_img.clamp_(0, 1)\n",
    "\n",
    "    total_loss, content_loss, style_loss = style_tranfer_(VGG19_pretrained, optimizer, target_img,\n",
    "                                                           content_features, final_style_features,\n",
    "                                                           style_layers, content_weight, style_weight)\n",
    "    if step % 100 == 99:\n",
    "        print(f\"Epoch [{step+1}/{STEPS}] Total loss: {total_loss.item():.6f} - \\\n",
    "                Content loss: {content_loss.item():.6f} - Style loss: {style_loss.item():.6f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_img.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5r1i6wU6i8-"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "imshow(target_img.detach(), title='Output Image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
