{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InputEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = InputEmbedding(vocab_size=10_000, embed_dim=512)\n",
    "embedded_output = embedding_layer(torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]))\n",
    "embedded_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 14).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = torch.exp(torch.arange(\n",
    "    0, 512, 2, dtype=torch.float) * -(math.log(10000) / 512)).unsqueeze(0)\n",
    "\n",
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 256])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = (position * div_term).unsqueeze(0)\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        N = 10000.0\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        self.embeddim = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # step = 2 ,vì pe cũng nhảy step = 2 \n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(math.log(N) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # thêm chiều 0 để 'pe' broadcasting với 'x'\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        init_position_embedding = self.embeddim(x)\n",
    "        # slicing 'pe' để có cùng shape với 'x'\n",
    "        return init_position_embedding + self.pe[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "pos_encoding_layer = PositionalEncoding(embed_dim=512, max_len=14)\n",
    "positions = torch.arange(0, 10).expand(3, 10)\n",
    "pos_encoding = pos_encoding_layer(positions)\n",
    "pos_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding, Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = torch.arange(0, 10).expand(3, 10)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module): \n",
    "    def __init__(self, vocab_size, embed_dim, max_length, device ='cpu'):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        self.token_embedding = InputEmbedding(embed_dim, vocab_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_dim, max_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        return self.token_embedding(x) + self.position_embedding(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock (nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dims, prob_drop):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dims, embed_dim)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, k, q, v):\n",
    "        attn_output, _ = self.multihead_attn(k, q, v)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        attn_output = self.layer_norm1(attn_output + q)\n",
    "        \n",
    "        ff_output = self.ffn(attn_output)\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        output = self.layer_norm2(ff_output + attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, n_layers, embed_dim, num_heads, ff_dims, prob_drop, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dims, prob_drop) for _ in range(n_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        x = self.token_pos_embedding(x)\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, x, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 512])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Encoder\n",
    "\n",
    "src = torch.randint(\n",
    "    high=2,\n",
    "    size=(2, 50),\n",
    "    dtype=torch.int64\n",
    ")\n",
    "\n",
    "test_x = TransformerEncoder(vocab_size=10_000, max_length=50, n_layers=8, embed_dim=512, num_heads=8, ff_dims=2048, prob_drop=0.1)(src)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformersBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dims, prob_drop):\n",
    "        super().__init__()\n",
    "        self.masked_multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dims, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dims, embed_dim, bias=True)\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm3 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, target, enc_output, tgt_mask, src_mask):\n",
    "        masked_attn_output, _ = self.masked_multihead_attn(\n",
    "            target, target, target, attn_mask=tgt_mask)\n",
    "        masked_attn_output = self.dropout1(masked_attn_output)\n",
    "        masked_attn_output = self.layer_norm1(masked_attn_output + target)\n",
    "\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            masked_attn_output, enc_output, enc_output, attn_mask=src_mask)\n",
    "        \n",
    "        attn_output = self.dropout2(attn_output)\n",
    "        attn_output = self.layer_norm2(attn_output + masked_attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        output = self.layer_norm3(ffn_output + attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim, num_heads, ff_dims, prob_drop, n_layers, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderTransformersBlock(embed_dim, num_heads, ff_dims, prob_drop) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        x = self.token_pos_embedding(x)\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, enc_output, tgt_mask, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, scr_vocab_size, tgt_vocab_size, max_length, embed_dim, num_heads, ff_dims, prob_drop, n_layers, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = TransformerEncoder(scr_vocab_size, max_length, n_layers, embed_dim, num_heads, ff_dims, prob_drop, device)\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size, max_length, embed_dim, num_heads, ff_dims, prob_drop, n_layers, device)\n",
    "\n",
    "        self.linear = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        encode_x = self.encoder(src)\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        decode_x = self.decoder(tgt, encode_x, src_mask, tgt_mask)\n",
    "        output = self.linear(decode_x)\n",
    "        return output\n",
    "        \n",
    "    def generate_mask(self, src_sequence, tgt_squence):\n",
    "        src_mask = torch.zeros(src_sequence.size(1), src_sequence.size(1)).to(self.device).type(torch.bool)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_squence.size(1), tgt_squence.size(\n",
    "            1)), diagonal=1).to(self.device)\n",
    "        tgt_mask = tgt_mask.type(torch.float).masked_fill(\n",
    "            tgt_mask == 1, float('-inf'))\n",
    "        \n",
    "        return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 1 required positional argument: 'n_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m ff_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscr_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(batch_size, max_length), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(batch_size, max_length), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: Transformer.__init__() missing 1 required positional argument: 'n_layers'"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "scr_vocab_size = 1000\n",
    "tgt_vocab_size = 2000\n",
    "embed_dim = 200\n",
    "max_length = 10\n",
    "num_layers = 2\n",
    "num_heads = 4  \n",
    "ff_dims = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(scr_vocab_size,tgt_vocab_size, max_length, embed_dim, num_heads, ff_dims, 0.1, num_layers, device=device).to(device)\n",
    "\n",
    "src = torch.randint(high=2, size=(batch_size, max_length), dtype=torch.int64).to(device)\n",
    "tgt = torch.randint(high=2, size=(batch_size, max_length), dtype=torch.int64).to(device)\n",
    "\n",
    "\n",
    "output = model(src, tgt)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
