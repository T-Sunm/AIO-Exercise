{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InputEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = InputEmbedding(vocab_size=10_000, embed_dim=512)\n",
    "embedded_output = embedding_layer(torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]))\n",
    "embedded_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 14).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = torch.exp(torch.arange(\n",
    "    0, 512, 2, dtype=torch.float) * -(math.log(10000) / 512)).unsqueeze(0)\n",
    "\n",
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 256])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = (position * div_term).unsqueeze(0)\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        N = 10000.0\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        self.embeddim = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # step = 2 ,vì pe cũng nhảy step = 2 \n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * -(math.log(N) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # thêm chiều 0 để 'pe' broadcasting với 'x'\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        init_position_embedding = self.embeddim(x)\n",
    "        # slicing 'pe' để có cùng shape với 'x'\n",
    "        return init_position_embedding + self.pe[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "pos_encoding_layer = PositionalEncoding(embed_dim=512, max_len=14)\n",
    "positions = torch.arange(0, 10).expand(3, 10)\n",
    "pos_encoding = pos_encoding_layer(positions)\n",
    "pos_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding, Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = torch.arange(0, 10).expand(3, 10)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module): \n",
    "    def __init__(self, vocab_size, embed_dim, max_length, device ='cpu'):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        self.token_embedding = InputEmbedding(embed_dim, vocab_size)\n",
    "        self.position_embedding = PositionalEncoding(embed_dim, max_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        return self.token_embedding(x) + self.position_embedding(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock (nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dims, prob_drop):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dims, embed_dim)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, k, q, v):\n",
    "        attn_output, _ = self.multihead_attn(k, q, v)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        attn_output = self.layer_norm1(attn_output + q)\n",
    "        \n",
    "        ff_output = self.ffn(attn_output)\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        output = self.layer_norm2(ff_output + attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, n_layers, embed_dim, num_heads, ff_dims, prob_drop, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dims, prob_drop) for _ in range(n_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        x = self.token_pos_embedding(x)\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, x, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 512])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Encoder\n",
    "\n",
    "src = torch.randint(\n",
    "    high=2,\n",
    "    size=(2, 50),\n",
    "    dtype=torch.int64\n",
    ")\n",
    "\n",
    "test_x = TransformerEncoder(vocab_size=10_000, max_length=50, n_layers=8, embed_dim=512, num_heads=8, ff_dims=2048, prob_drop=0.1)(src)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformersBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dims, prob_drop):\n",
    "        super().__init__(embed_dim, num_heads, ff_dims, prob_drop)\n",
    "        self.masked_multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, mask=True)\n",
    "        self.dropout1 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout2 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dims, embed_dim)\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(prob_drop)\n",
    "        self.layer_norm3 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, target, enc_output, tgt_mask, src_mask):\n",
    "        masked_attn_output, _ = self.masked_multihead_attn(\n",
    "            target, target, target, attn_mask=tgt_mask)\n",
    "        masked_attn_output = self.dropout1(masked_attn_output)\n",
    "        masked_attn_output = self.layer_norm1(masked_attn_output + target)\n",
    "\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            masked_attn_output, enc_output, enc_output, attn_mask=src_mask)\n",
    "        \n",
    "        attn_output = self.dropout2(attn_output)\n",
    "        attn_output = self.layer_norm2(attn_output + masked_attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        output = self.layer_norm3(ffn_output + attn_output)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim, num_heads, ff_dims, prob_drop, n_layers, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderTransformersBlock(embed_dim, num_heads, ff_dims, prob_drop) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask, src_mask):\n",
    "        x = self.token_pos_embedding(x)\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, enc_output, tgt_mask, src_mask)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
