{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import lib and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of samples : 3914\n"
     ]
    }
   ],
   "source": [
    "# install library\n",
    "# pip install evaluate\n",
    "\n",
    "# import library\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "\n",
    "# load tree bank dataset\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print(\" Number of samples :\", len(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join',\n",
       "       'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.',\n",
       "       '29', '.'], dtype='<U12')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mapping(sentence_tags: List[List[str]]):\n",
    "    tags = set()\n",
    "    for sen_tags in sentence_tags:\n",
    "        for tag in sen_tags:\n",
    "            tags.add(tag)\n",
    "\n",
    "    label2id = {tag: i for i, tag in enumerate(tags)}\n",
    "    label2id['<PAD>'] = len(label2id)\n",
    "    id2label = {i: tag for tag, i in label2id.items()}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = get_label_mapping(sentence_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7 - 0.15 - 0.15\n",
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(sentences, sentence_tags, test_size=0.3)\n",
    "test_sentences, val_sentences, test_tags, val_tags = train_test_split(\n",
    "    test_sentences, test_tags, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    # use_fast để sử dụng tokenize nhanh\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "class postagging_dataset(Dataset):\n",
    "    def __init__(self, sentences: List[List[str]], tags: List[List[str]], tokenizer, label2id, max_length = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label2id = label2id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tag = self.tags[idx]\n",
    "\n",
    "        encode_sen = self.tokenizer.convert_tokens_to_ids(sentence)\n",
    "        encode_tag = [self.label2id[t] for t in tag]\n",
    "        attention_mask = [1] * len(encode_sen)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": self.pad_and_truncate(encode_sen, pad_id=self.tokenizer.pad_token_id),\n",
    "            \"labels\": self.pad_and_truncate(encode_tag, pad_id=self.label2id[\"<PAD>\"]),\n",
    "            \"attention_mask\": self.pad_and_truncate(attention_mask, pad_id=0)\n",
    "        }\n",
    "    \n",
    "    def pad_and_truncate(self, encoded, pad_id):\n",
    "        if len(encoded) < self.max_length:\n",
    "            padding = [pad_id] * (self.max_length - len(encoded))\n",
    "            encoded = encoded + padding\n",
    "        else:\n",
    "            encoded = encoded[:self.max_length]\n",
    "        return encoded  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = postagging_dataset(train_sentences, train_tags, tokenizer, label2id)\n",
    "val_data = postagging_dataset(\n",
    "    val_sentences, val_tags, tokenizer, label2id)\n",
    "test_data = postagging_dataset(\n",
    "    train_sentences, train_tags, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([46, 768]) in the checkpoint and torch.Size([47, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([46]) in the checkpoint and torch.Size([47]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ví dụ về ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " [[ True  True  True False False]\n",
      " [ True  True  True  True False]\n",
      " [ True  True  True  True  True]]\n",
      "\n",
      "Predicted Classes (after argmax):\n",
      " [[1 0 2 0 0]\n",
      " [3 2 0 0 0]\n",
      " [3 2 0 0 0]]\n",
      "\n",
      "Labels:\n",
      " [[0 1 2 5 5]\n",
      " [3 4 0 1 5]\n",
      " [2 3 4 0 1]]\n",
      "\n",
      "Masked Predictions:\n",
      " [1 0 2 3 2 0 0 3 2 0 0 0]\n",
      "\n",
      "Masked Labels:\n",
      " [0 1 2 3 4 0 1 2 3 4 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_label = 5\n",
    "labels = np.array([\n",
    "    [0, 1, 2, 5, 5],\n",
    "    [3, 4, 0, 1, 5],\n",
    "    [2, 3, 4, 0, 1]\n",
    "])\n",
    "\n",
    "predictions_logits = np.array([  # Original logits (before argmax) - let's rename for clarity\n",
    "    [\n",
    "        [0.1, 0.8, 0.2, 0.05, 0.05],\n",
    "        [0.6, 0.1, 0.1, 0.1, 0.1],\n",
    "        [0.0, 0.0, 0.7, 0.2, 0.1],\n",
    "        # Logits for padding token (doesn't really matter)\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2]  # Logits for padding token\n",
    "    ],\n",
    "    [\n",
    "        [0.2, 0.3, 0.1, 0.4, 0.0],\n",
    "        [0.1, 0.0, 0.8, 0.05, 0.05],\n",
    "        [0.5, 0.2, 0.1, 0.1, 0.1],\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2],  # Logits for padding token\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2]  # Logits for padding token\n",
    "    ],\n",
    "    [\n",
    "        [0.2, 0.3, 0.1, 0.4, 0.0],\n",
    "        [0.1, 0.0, 0.8, 0.05, 0.05],\n",
    "        [0.5, 0.2, 0.1, 0.1, 0.1],\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "    ]\n",
    "])\n",
    "\n",
    "\n",
    "mask = labels != ignore_label\n",
    "predicted_classes = np.argmax(predictions_logits, axis=-1)  # Apply argmax\n",
    "\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"\\nPredicted Classes (after argmax):\\n\", predicted_classes)\n",
    "print(\"\\nLabels:\\n\", labels)\n",
    "\n",
    "masked_predictions = predicted_classes[mask]\n",
    "masked_labels = labels[mask]\n",
    "\n",
    "print(\"\\nMasked Predictions:\\n\", masked_predictions)\n",
    "print(\"\\nMasked Labels:\\n\", masked_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# vì mình đạt pad = len(label2id) nên mình sẽ bỏ qua label này\n",
    "ignore_label = label2id[\"<PAD>\"]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    # bỏ qua padding\n",
    "    mask = labels != ignore_label\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions[mask], references=labels[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20876\\1037861419.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoonlig73\u001b[0m (\u001b[33mminhdeptrai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Asus\\AIO\\AIO-Exercise\\module8\\week1\\wandb\\run-20250203_183039-r0m5zjvy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minhdeptrai/huggingface/runs/r0m5zjvy' target=\"_blank\">out_dir</a></strong> to <a href='https://wandb.ai/minhdeptrai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minhdeptrai/huggingface' target=\"_blank\">https://wandb.ai/minhdeptrai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minhdeptrai/huggingface/runs/r0m5zjvy' target=\"_blank\">https://wandb.ai/minhdeptrai/huggingface/runs/r0m5zjvy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 75/1720 [10:58<4:16:18,  9.35s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\accelerate\\accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"out_dir\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[-2.2665e-01, -1.1905e+00,  6.3013e-01, -3.3882e-01, -4.6584e-01,\n",
      "           3.5006e-01,  3.1260e-01, -2.9735e-05,  1.1614e-01, -3.4921e-01,\n",
      "          -8.9358e-02, -2.4233e-01, -6.4159e-01,  6.0555e-01, -8.0269e-02,\n",
      "          -1.8956e-01, -6.1509e-01, -3.6126e-01,  2.7667e-01, -6.5976e-02,\n",
      "          -2.8197e-02, -2.8431e-01, -9.6452e-02, -1.3094e-02, -4.0846e-01,\n",
      "          -3.1601e-01, -1.7764e-01, -3.1447e-01,  1.1169e+00, -3.6802e-01,\n",
      "          -3.9152e-01,  5.3389e-02, -1.3965e-01, -4.8342e-01, -4.3083e-01,\n",
      "          -2.9877e-01,  1.1248e-01, -1.0130e-01, -3.6021e-01, -7.0039e-01,\n",
      "          -9.2520e-02,  3.9546e-02,  3.0646e-01,  9.2590e-02, -5.4446e-01,\n",
      "          -5.4229e-01,  5.4034e+00],\n",
      "         [-7.3233e-01, -1.0640e+00,  3.1379e-01, -2.4655e-01, -4.2472e-01,\n",
      "           1.7328e+00,  6.7520e-01,  1.7756e-01,  1.2829e+00, -6.3129e-01,\n",
      "          -6.6374e-02, -5.1448e-01, -1.3419e-01,  8.6598e-01,  9.4508e-01,\n",
      "          -3.3203e-01, -8.0802e-01,  1.8725e-01,  6.7131e-01,  2.2672e-02,\n",
      "          -7.9927e-01, -4.0648e-01, -3.6832e-01, -7.7146e-01, -4.2319e-01,\n",
      "          -1.2618e+00,  5.7725e-01, -4.0854e-01,  8.3064e-01, -2.7384e-01,\n",
      "          -5.8688e-01, -3.2136e-02, -7.5919e-01, -8.1725e-01,  2.0846e-01,\n",
      "          -1.0934e-01,  3.7079e-01,  1.9964e-01, -6.0184e-01, -4.8841e-01,\n",
      "          -1.2246e+00,  7.1224e-01,  8.0039e-02,  1.3303e+00,  1.1708e-01,\n",
      "           1.1480e+00,  1.0115e+00],\n",
      "         [-7.9987e-01, -7.7118e-01,  2.5678e-02, -4.8164e-01, -8.8608e-01,\n",
      "           1.7827e+00, -1.3103e-01, -4.5001e-01,  1.7622e+00, -5.4672e-01,\n",
      "          -9.4368e-02, -2.2352e-01,  4.5911e-01,  8.2834e-01,  1.3498e-01,\n",
      "          -2.8276e-01, -1.0453e+00, -2.8457e-01,  2.1798e+00,  1.5297e-01,\n",
      "          -1.0995e+00, -3.6939e-01, -2.9981e-01, -3.7617e-01, -3.5006e-01,\n",
      "          -8.7547e-01,  3.6933e-01,  4.1184e-01, -1.6693e-03, -5.2125e-01,\n",
      "          -5.2309e-01, -4.9665e-01, -5.9384e-01, -1.2021e+00,  2.2931e-01,\n",
      "          -6.0810e-02,  9.0703e-01,  5.6125e-01, -1.0250e+00, -3.3933e-01,\n",
      "          -6.3173e-01,  7.9061e-01,  2.0558e-01,  6.6128e-01,  4.3935e-04,\n",
      "           1.3812e+00, -3.4547e-01],\n",
      "         [ 3.8435e-01, -2.7886e-01,  6.5500e-01, -1.8941e-01,  5.5191e-02,\n",
      "           5.6627e-01,  3.0449e-01, -3.1385e-01, -2.8458e-01, -4.0567e-01,\n",
      "          -1.7315e-03, -1.2044e-01, -3.5212e-01, -1.2200e-01,  4.5520e-04,\n",
      "          -1.2022e-01, -3.4992e-01, -7.1160e-02, -3.7457e-01, -2.9797e-01,\n",
      "          -7.5200e-01, -3.2002e-01, -5.3175e-02, -3.3416e-01, -2.6577e-01,\n",
      "          -6.8455e-01, -5.9743e-01, -3.1669e-01,  5.2264e+00,  3.6698e-02,\n",
      "          -8.1299e-01,  5.7920e-01,  3.0056e-02, -1.1383e+00, -8.1298e-02,\n",
      "          -1.4293e-01,  8.3638e-01, -5.6333e-01,  5.9887e-01,  6.8775e-01,\n",
      "          -1.0058e+00, -2.5383e-01, -2.7353e-02,  9.4987e-01,  3.8926e-01,\n",
      "           9.3624e-01,  6.1828e-01],\n",
      "         [-5.0970e-01, -4.3205e-01,  5.4454e-01, -2.8045e-01, -4.2268e-01,\n",
      "           1.2354e+00,  4.3039e-01, -7.3993e-01,  1.1755e-01, -7.6853e-01,\n",
      "          -1.1813e-01,  5.9549e-02, -1.9519e-01, -6.0423e-01, -3.0282e-01,\n",
      "          -4.6179e-01, -3.7065e-01, -3.7939e-01, -1.9857e-01, -7.7571e-01,\n",
      "          -1.3599e+00, -5.3996e-01, -3.1568e-03, -7.2177e-01, -4.6054e-01,\n",
      "          -7.9278e-01, -7.1027e-02, -6.3696e-01,  5.1071e-01, -8.0610e-01,\n",
      "          -9.4639e-01, -3.7872e-01, -1.9463e-01, -2.9263e-01, -1.1517e+00,\n",
      "           3.5185e-01, -2.0445e-02,  1.7171e-01, -4.5357e-01, -4.6218e-01,\n",
      "          -7.5496e-01, -2.5869e-01, -6.2875e-01,  3.5895e-01,  5.5674e-02,\n",
      "           3.8613e+00,  8.1058e-01],\n",
      "         [ 4.1809e-01, -8.1194e-01,  5.0704e+00, -1.6863e-01,  2.9691e-01,\n",
      "           2.6650e-01,  1.3952e-01, -4.3587e-01, -4.4119e-01, -1.0318e+00,\n",
      "           6.7942e-01, -5.3285e-02,  4.8062e-01,  2.1284e-01, -8.5560e-01,\n",
      "          -6.5663e-01,  5.6882e-02, -4.3581e-01, -2.1566e-01, -3.4802e-01,\n",
      "          -7.3286e-01, -3.8999e-02,  6.8688e-01, -3.2289e-02, -2.3661e-01,\n",
      "          -6.9340e-01,  3.6802e-01, -3.4090e-01,  5.4297e-01,  9.1556e-02,\n",
      "          -4.5580e-01, -3.7958e-01, -5.3647e-01, -5.1401e-01, -5.7093e-01,\n",
      "           1.1798e-01,  3.0952e-01, -2.0482e-01,  1.4584e-01, -1.2797e-01,\n",
      "           7.9220e-02,  5.5610e-01,  1.2836e-01,  2.9238e-01,  5.6338e-01,\n",
      "           8.4377e-01,  2.4484e-01],\n",
      "         [-4.8406e-01, -6.8856e-01,  1.9374e-01, -5.1523e-01, -4.2796e-01,\n",
      "           3.3714e-01, -2.2623e-02,  5.0130e-01,  1.4138e+00,  2.1281e-01,\n",
      "          -2.8595e-01, -1.1555e+00, -6.5554e-01, -3.5896e-01,  3.3126e-01,\n",
      "          -5.8950e-01, -1.3228e-01, -5.0739e-01,  3.0008e-01, -3.8144e-01,\n",
      "          -8.8827e-01, -5.0274e-02, -6.9060e-02, -5.0499e-01, -3.8937e-01,\n",
      "          -7.7500e-01,  2.5477e-01, -1.1537e+00,  4.9797e-02, -6.5338e-01,\n",
      "          -4.7351e-01, -1.1060e+00, -6.3533e-01,  3.9425e-01,  1.2307e-01,\n",
      "          -8.0128e-02,  2.3979e+00,  3.1827e-01, -1.0072e+00, -8.9975e-01,\n",
      "          -4.0309e-01,  3.7562e-01, -1.1343e+00,  3.4967e-01, -1.0423e-01,\n",
      "           1.5997e+00,  3.0991e-02],\n",
      "         [-1.3332e-01, -6.0881e-01,  6.9694e-01,  7.2531e-02, -4.8935e-01,\n",
      "           1.2116e+00, -1.4039e-01, -4.5811e-01, -1.0616e-01, -4.3438e-01,\n",
      "          -3.4442e-01,  1.7598e-01, -3.2854e-02, -5.7031e-01, -4.8888e-01,\n",
      "          -5.3172e-01, -5.3589e-01, -4.9520e-01, -1.6169e-02,  1.2171e-01,\n",
      "          -1.2153e+00, -8.1428e-01,  6.3405e-01,  1.8395e-02, -5.7474e-01,\n",
      "          -2.8956e-01, -1.4513e-01, -7.5115e-01,  2.4686e-01, -4.0192e-01,\n",
      "          -6.5653e-01, -1.5074e-01, -1.9716e-01,  2.1803e-01, -5.2911e-01,\n",
      "           6.7287e-01,  9.1398e-01,  7.0095e-01, -6.3276e-01, -4.9099e-01,\n",
      "          -1.1443e+00, -4.2864e-02, -8.6131e-01,  4.4194e-01, -2.0352e-03,\n",
      "           2.9169e+00,  7.5750e-01]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<PAD> NNS -NONE- DT NN IN JJ NN '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "test_sentence = \"We are exploring the topic of deep learning \"\n",
    "input = torch.as_tensor(\n",
    "    [tokenizer.convert_tokens_to_ids(test_sentence.split())])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input = input.to(device)\n",
    "\n",
    "# prediction\n",
    "outputs = model(input)\n",
    "_, preds = torch.max(outputs.logits, -1)\n",
    "preds = preds[0].cpu().numpy()\n",
    "\n",
    "# decode\n",
    "pred_tags = \"\"\n",
    "for pred in preds:\n",
    "    pred_tags += id2label[pred] + \" \"\n",
    "pred_tags  # = > PRP VBP RB DT NN IN JJ NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
