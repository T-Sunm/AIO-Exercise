{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libary và load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, pre_tokenizers, trainers, models\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"thainq107/iwslt2015-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Rachel Pike : The science behind a climate headline',\n",
       " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined toknize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-based Tokenizer\n",
    "tokenizer_en = Tokenizer(models.WordLevel(unk_token=\"<unk>\")) # Tạo 2 tokenizer\n",
    "tokenizer_vi = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace() # Tách dựa trên khoảng trắng\n",
    "tokenizer_vi.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    vocab_size=15000, \n",
    "    min_frequency=2,  # xác định tần suất xuất hiện tối thiểu để được đưa vào vocab\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer_en.train_from_iterator(ds[\"train\"][\"en\"], trainer) # Vừa xây dựng vocab vừa tokenize\n",
    "tokenizer_vi.train_from_iterator(ds[\"train\"][\"vi\"], trainer)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_en.save(\"tokenizer_en.json\")\n",
    "tokenizer_vi.save(\"tokenizer_vi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 75\n",
    "from transformers import  PreTrainedTokenizerFast\n",
    "tokenizer_en = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer_en.json\", unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos\")\n",
    "tokenizer_vi = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer_vi.json\",\n",
    "                                       unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens_encoder = tokenizer_en.added_tokens_encoder\n",
    "added_tokens_encoder['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3497.56 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3491.01 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 4240.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    src_text = examples[\"en\"]\n",
    "    tgt_text = [\"<bos> \" + text + \" <eos>\" for text in examples[\"vi\"]]\n",
    "\n",
    "    src_encodings = tokenizer_en(\n",
    "        src_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    tgt_encodings = tokenizer_vi(\n",
    "        tgt_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "    return {\n",
    "        # Trả về list, không phải tensor\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],  # Trả về list, không phải tensor\n",
    "    }\n",
    "\n",
    "\n",
    "# Áp dụng map() và set_format(\"torch\") để tự động chuyển thành tensor\n",
    "preprocessed_train = ds['train'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "preprocessed_val = ds['validation'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "preprocessed_test = ds['test'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "\n",
    "# Định dạng dữ liệu dưới dạng torch.Tensor\n",
    "preprocessed_train.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "preprocessed_val.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "preprocessed_test.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([6675,    1,   57,   60,  339,  604,   13,  744, 5643,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0]), 'labels': tensor([   2, 1960,   66, 1157,  131,    8,  376,  113,   38,  417,  735,    3,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])}\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra đầu ra\n",
    "print(preprocessed_train[0])\n",
    "print(type(preprocessed_train[0][\"input_ids\"]))  # Phải là torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class Seq2SeqRNNConfig(PretrainedConfig):\n",
    "    def __init__(self, vocab_size_src = 10000, vocab_size_tgt = 10000, embedding_dim = 128, hidden_size = 128, dropout = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size_src = vocab_size_src\n",
    "        self.vocab_size_tgt = vocab_size_tgt\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size=10000, embedding_dim=128, hidden_size=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size=10000, embedding_dim=128, hidden_size=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_decoder):\n",
    "        embedded = self.embedding(x)\n",
    "        output, last_hidden = self.gru(embedded, hidden_decoder)\n",
    "        output = self.out(output)\n",
    "        return output, last_hidden\n",
    "    \n",
    "class Seq2SeqRNNmodel(PreTrainedModel):\n",
    "    def __init__(self, config: Seq2SeqRNNConfig, tokenizer_en: PreTrainedTokenizerFast):\n",
    "        super().__init__(config)\n",
    "        self.encoder = EncoderRNN(config.vocab_size_src, config.embedding_dim, config.hidden_size, config.dropout).to(device)\n",
    "        self.decoder = DecoderRNN(\n",
    "            config.vocab_size_tgt, config.embedding_dim, config.hidden_size, config.dropout).to(device)\n",
    "        self.bos_idx = tokenizer_en.added_tokens_encoder['<bos>']\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=tokenizer_en.added_tokens_encoder['<pad>'])\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        batch_size, seq_len  = labels.size()\n",
    "        encoder_output, encoder_hidden = self.encoder(input_ids)\n",
    "        # print(encoder_output.shape, encoder_hidden.shape)\n",
    "\n",
    "        inputs_decoder = torch.full((batch_size, 1), self.bos_idx, dtype=torch.long).to(input_ids.device) # BOS token\n",
    "        hidden_decoder = encoder_hidden\n",
    "        output_decoder_lst = []\n",
    "\n",
    "        # teacher forcing\n",
    "        for i in range(seq_len):\n",
    "            output_decoder, hidden_decoder = self.decoder(inputs_decoder, hidden_decoder)\n",
    "            # print(output_decoder.shape, hidden_decoder.shape)\n",
    "            inputs_decoder = labels[:, i].unsqueeze(1)\n",
    "            output_decoder_lst.append(output_decoder)\n",
    "\n",
    "        # print(len(output_decoder_lst), output_decoder_lst[0].shape)\n",
    "        logits = torch.cat(output_decoder_lst, dim=1) # (batch_size, seq_len, vocab_size_tgt)\n",
    "        loss = self.loss_fn(logits.permute(0, 2, 1), labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(9.2516, grad_fn=<NllLoss2DBackward0>),\n",
       " 'logits': tensor([[[-0.0581, -0.1600,  0.0921,  ...,  0.2343, -0.4025, -0.4931],\n",
       "          [-0.0602, -0.0598,  0.0117,  ...,  0.2292, -0.3739, -0.3102],\n",
       "          [ 0.0197, -0.0962, -0.1172,  ..., -0.0404, -0.2372, -0.0765],\n",
       "          ...,\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316],\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316],\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316]],\n",
       " \n",
       "         [[-0.0581, -0.1601,  0.0921,  ...,  0.2342, -0.4017, -0.4931],\n",
       "          [-0.0602, -0.0598,  0.0117,  ...,  0.2291, -0.3734, -0.3102],\n",
       "          [-0.1354, -0.0927,  0.2444,  ...,  0.0566,  0.0292, -0.4020],\n",
       "          ...,\n",
       "          [ 0.1929, -0.1134, -0.0746,  ..., -0.3125,  0.1782,  0.2041],\n",
       "          [ 0.1298, -0.0669, -0.1370,  ..., -0.2732,  0.4786, -0.0553],\n",
       "          [ 0.1379,  0.1780, -0.0321,  ..., -0.0070,  0.5041, -0.1037]],\n",
       " \n",
       "         [[-0.0581, -0.1600,  0.0921,  ...,  0.2343, -0.4025, -0.4931],\n",
       "          [-0.0602, -0.0598,  0.0117,  ...,  0.2292, -0.3739, -0.3102],\n",
       "          [ 0.1254,  0.0177,  0.1043,  ...,  0.1531, -0.4084, -0.2846],\n",
       "          ...,\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316],\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316],\n",
       "          [ 0.0766,  0.1735, -0.1656,  ...,  0.3689, -0.1773,  0.6316]]],\n",
       "        grad_fn=<CatBackward0>)}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Seq2SeqRNNConfig()\n",
    "model = Seq2SeqRNNmodel(config, tokenizer_en)\n",
    "model.forward(preprocessed_train[0: 3]['input_ids'],\n",
    "              preprocessed_train[0: 3]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./en-vi-machine-translation\",\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    num_train_epochs=25,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_train,\n",
    "    eval_dataset=preprocessed_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu==2.5.1\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu==2.5.1)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu==2.5.1)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (1.26.3)\n",
      "Requirement already satisfied: colorama in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (0.4.6)\n",
      "Collecting lxml (from sacrebleu==2.5.1)\n",
      "  Downloading lxml-5.3.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from portalocker->sacrebleu==2.5.1) (307)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.3.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.3.1 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sacrebleu==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: When I was little , I thought my country was the best on the planet , and I grew up singing a song called & quot ; Nothing To <unk> . & quot ; <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Target: <bos> Khi tôi còn nhỏ , Tôi nghĩ rằng <unk> Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài & quot ; Chúng ta chẳng có gì phải ghen tị . & quot ; <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Predict: proton 1959 Ba bin Agnes nhào Treasure hạn Piano đãng hợm Húp Mad Nuôi ôtô xuân MySpace dốt nhiện Frederic Libby lộn Tứ JB JR Vietnam Tyler 380 quyệt Aldo xê Tweets giành Tuổi Idol Collusion Ellen nếu Treatment Williamson ruột Ở Khuôn Sergei Frankie bợm thả Dennett dollar Chìa JB iO giương Plato GIờ virút Woody chùa nếu giấy Cruz sơm thầu hâu Bitcoin truồng đợt rọc Wi Hưng Chung Chung lương Brian Temple\n",
      "BLEU Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6652\\442388959.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src = torch.tensor(src).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "def greedy_decode(model, src, max_len, tokenizer, device=\"cpu\"):\n",
    "    src = torch.tensor(src).unsqueeze(0).to(device)\n",
    "    memory, hidden = model.encoder(src)\n",
    "    y_start = torch.full((1, 1), tokenizer.added_tokens_encoder[\"<bos>\"], dtype=torch.long).to(device)\n",
    "    output = []\n",
    "\n",
    "    # teacher forcing\n",
    "    for i in range(max_len):\n",
    "        output_decoder, hidden = model.decoder(y_start, hidden)\n",
    "        _, next_word = output_decoder.max(dim=-1)\n",
    "        y_start = next_word\n",
    "        output.append(next_word.item())\n",
    "\n",
    "        if next_word.item() == tokenizer.added_tokens_encoder[\"<eos>\"]:\n",
    "            break\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "def translate():\n",
    "    model.eval()\n",
    "    src = preprocessed_test[0][\"input_ids\"]\n",
    "    tgt = preprocessed_test[0][\"labels\"]\n",
    "    output = greedy_decode(model, src, MAX_LENGTH,\n",
    "                           tokenizer_vi, device=\"cpu\")\n",
    "    \n",
    "    print(\"Input:\", tokenizer_en.decode(src))\n",
    "    print(\"Target:\", tokenizer_vi.decode(tgt))\n",
    "    print(\"Predict:\", tokenizer_vi.decode(output))\n",
    "\n",
    "    bleu_score = sacrebleu.corpus_bleu(\n",
    "        [tokenizer_vi.decode(output)], [[tokenizer_vi.decode(tgt)]], force=True).score\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "translate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
