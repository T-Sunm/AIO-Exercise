{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libary và load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, pre_tokenizers, trainers, models\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"thainq107/iwslt2015-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Rachel Pike : The science behind a climate headline',\n",
       " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined toknize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-based Tokenizer\n",
    "tokenizer_en = Tokenizer(models.WordLevel(unk_token=\"<unk>\")) # Tạo 2 tokenizer\n",
    "tokenizer_vi = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace() # Tách dựa trên khoảng trắng\n",
    "tokenizer_vi.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    vocab_size=15000, \n",
    "    min_frequency=2,  # xác định tần suất xuất hiện tối thiểu để được đưa vào vocab\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer_en.train_from_iterator(ds[\"train\"][\"en\"], trainer) # Vừa xây dựng vocab vừa tokenize\n",
    "tokenizer_vi.train_from_iterator(ds[\"train\"][\"vi\"], trainer)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_en.save(\"./transformer/tokenizer_en.json\")\n",
    "tokenizer_vi.save(\"./transformer/tokenizer_vi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 75\n",
    "from transformers import  PreTrainedTokenizerFast\n",
    "tokenizer_en = PreTrainedTokenizerFast(tokenizer_file=\"./transformer/tokenizer_en.json\", unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos\")\n",
    "tokenizer_vi = PreTrainedTokenizerFast(tokenizer_file=\"./transformer/tokenizer_vi.json\",\n",
    "                                       unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15001"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_src = len(tokenizer_en.get_vocab())\n",
    "vocab_size_tgt = len(tokenizer_vi.get_vocab())\n",
    "vocab_size_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens_encoder = tokenizer_en.added_tokens_encoder\n",
    "added_tokens_encoder['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    src_text = examples[\"en\"]\n",
    "    tgt_text = [\"<bos> \" + text + \" <eos>\" for text in examples[\"vi\"]]\n",
    "\n",
    "    src_encodings = tokenizer_en(\n",
    "        src_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    tgt_encodings = tokenizer_vi(\n",
    "        tgt_text, truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "    return {\n",
    "        # Trả về list, không phải tensor\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],  # Trả về list, không phải tensor\n",
    "    }\n",
    "\n",
    "\n",
    "# Áp dụng map() và set_format(\"torch\") để tự động chuyển thành tensor\n",
    "preprocessed_train = ds['train'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "preprocessed_val = ds['validation'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "preprocessed_test = ds['test'].select(\n",
    "    range(100)).map(preprocess_function, batched=True)\n",
    "\n",
    "# Định dạng dữ liệu dưới dạng torch.Tensor\n",
    "preprocessed_train.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "preprocessed_val.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "preprocessed_test.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([6675,    1,   57,   60,  339,  604,   13,  744, 5643,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0]), 'labels': tensor([   2, 1960,   66, 1157,  131,    8,  376,  113,   38,  417,  735,    3,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])}\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra đầu ra\n",
    "print(preprocessed_train[0])\n",
    "print(type(preprocessed_train[0][\"input_ids\"]))  # Phải là torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Mask:\n",
      " tensor([[False, False, False,  True,  True]])\n",
      "\n",
      "Attention Scores trước khi áp dụng mask:\n",
      " tensor([[5.6528e-01, 1.5462e-01, 6.3104e-01, 8.0898e-01, 1.9997e-01],\n",
      "        [2.5445e-01, 2.3181e-01, 5.8909e-01, 9.9554e-01, 3.3283e-01],\n",
      "        [2.6358e-01, 4.9669e-01, 5.1804e-02, 5.0553e-01, 9.1042e-01],\n",
      "        [2.4338e-01, 3.2681e-01, 5.3528e-01, 4.2900e-01, 3.2967e-04],\n",
      "        [6.0340e-01, 6.4141e-01, 1.9927e-01, 4.3930e-01, 8.6750e-01]])\n",
      "\n",
      "Attention Scores sau khi áp dụng mask:\n",
      " tensor([[0.5653, 0.1546, 0.6310,   -inf,   -inf],\n",
      "        [0.2545, 0.2318, 0.5891,   -inf,   -inf],\n",
      "        [0.2636, 0.4967, 0.0518,   -inf,   -inf],\n",
      "        [0.2434, 0.3268, 0.5353,   -inf,   -inf],\n",
      "        [0.6034, 0.6414, 0.1993,   -inf,   -inf]])\n",
      "\n",
      "Attention Weights sau Softmax:\n",
      " tensor([[0.3661, 0.2428, 0.3910, 0.0000, 0.0000],\n",
      "        [0.2963, 0.2897, 0.4140, 0.0000, 0.0000],\n",
      "        [0.3256, 0.4110, 0.2634, 0.0000, 0.0000],\n",
      "        [0.2919, 0.3173, 0.3908, 0.0000, 0.0000],\n",
      "        [0.3695, 0.3838, 0.2467, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Từ điển ánh xạ từ thành ID\n",
    "vocab = {\"Hello\": 1, \"world\": 2, \"!\": 3, \"<PAD>\": 0}\n",
    "\n",
    "# Câu đã tokenized\n",
    "sentence = [\"Hello\", \"world\", \"!\", \"<PAD>\", \"<PAD>\"]\n",
    "sentence_ids = torch.tensor([[vocab[word] for word in sentence]])\n",
    "\n",
    "# Tạo Padding Mask (True nếu là <PAD>)\n",
    "padding_mask = (sentence_ids == vocab[\"<PAD>\"])\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "\n",
    "# Tạo ma trận Attention Scores ngẫu nhiên (giả lập)\n",
    "seq_len = sentence_ids.shape[1]\n",
    "attention_scores = torch.rand(seq_len, seq_len)  # Ma trận Attention (5x5)\n",
    "\n",
    "print(\"\\nAttention Scores trước khi áp dụng mask:\\n\", attention_scores)\n",
    "\n",
    "# Áp dụng padding mask: Gán -inf cho vị trí <PAD> để loại bỏ khi tính softmax\n",
    "attention_scores = attention_scores.masked_fill(padding_mask, float('-inf'))\n",
    "\n",
    "print(\"\\nAttention Scores sau khi áp dụng mask:\\n\", attention_scores)\n",
    "\n",
    "# Tính Softmax để thấy sự khác biệt\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "print(\"\\nAttention Weights sau Softmax:\\n\", attention_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = torch.triu(torch.ones(sz, sz, device=device)) == 1  \n",
    "    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, tokenizer_en, tokenizer_vi, device):\n",
    "    # src = tgt = [batch_size, seq_len]\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    mask_decoder = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    mask_encoder = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = (src == tokenizer_en.added_tokens_encoder['<pad>'])\n",
    "    tgt_padding_mask = (tgt == tokenizer_vi.added_tokens_encoder['<pad>'])\n",
    "    return mask_encoder, mask_decoder,src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class Seq2SeqTransformerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size_src=15000,\n",
    "        vocab_size_tgt=15000,\n",
    "        max_seq_length=50,\n",
    "        d_model=256,\n",
    "        num_heads=8,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size_src = vocab_size_src\n",
    "        self.vocab_size_tgt = vocab_size_tgt\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, config:Seq2SeqTransformerConfig):\n",
    "        super().__init__(*args, **kwargs)\n",
    "class Seq2SeqTransformer(PreTrainedModel):\n",
    "    def __init__(self, config: Seq2SeqTransformerConfig, tokenizer_vi: PreTrainedTokenizerFast):\n",
    "        super().__init__(config)\n",
    "        self.embedding_src = nn.Embedding(config.vocab_size_src, config.d_model)\n",
    "        self.embedding_tgt = nn.Embedding(config.vocab_size_tgt, config.d_model)\n",
    "\n",
    "        self.position_embedding_src = nn.Embedding(\n",
    "            config.max_seq_length, config.d_model\n",
    "        )\n",
    "        self.position_embedding_tgt = nn.Embedding(\n",
    "            config.max_seq_length, config.d_model\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.num_heads,\n",
    "            num_decoder_layers=config.num_layers,\n",
    "            num_encoder_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        ) # --> [B, Seq_length, E]\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            config.d_model, config.vocab_size_tgt\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=tokenizer_vi.added_tokens_encoder['<pad>'])\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        # teacher forcing\n",
    "        tgt_input = labels[:, :-1]\n",
    "        tgt_output = labels[:, 1:]\n",
    "\n",
    "        batch_size , seq_len_src = input_ids.shape\n",
    "        _, seq_len_tgt = tgt_input.shape\n",
    "\n",
    "        src_positions = torch.arange(seq_len_src, device=input_ids.device).unsqueeze(0)\n",
    "        tgt_positions = torch.arange(\n",
    "            seq_len_tgt, device=tgt_input.device).unsqueeze(0)\n",
    "\n",
    "        src_embedded = self.embedding_src(input_ids) + self.position_embedding_src(src_positions) # broad casting để cộng lại\n",
    "        tgt_embedded = self.embedding_tgt(tgt_input) + self.position_embedding_tgt(tgt_positions)\n",
    "        src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask = create_mask(\n",
    "            input_ids, tgt_input, tokenizer_en, tokenizer_vi, device=input_ids.device)\n",
    "\n",
    "        outs = self.transformer(\n",
    "            src_embedded, tgt_embedded, src_mask, tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(outs)\n",
    "        loss = self.loss_fn(logits.permute(0, 2, 1), tgt_output)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "    \n",
    "    def encoder(self, src, src_mask):\n",
    "        _, seq_len_src = src.shape\n",
    "        src_positions = torch.arange(seq_len_src, device=src.device).unsqueeze(0)\n",
    "        src_embedded = self.embedding_src(src) + self.position_embedding_src(src_positions)\n",
    "        return self.transformer.encoder(src_embedded, src_mask)\n",
    "\n",
    "    def decoder(self, tgt, encoder_output, tgt_mask):\n",
    "        _, seq_len_tgt = tgt.shape\n",
    "        tgt_positions = torch.arange(seq_len_tgt, device=tgt.device).unsqueeze(0)\n",
    "        tgt_embedded = self.embedding_tgt(tgt) + self.position_embedding_tgt(tgt_positions)\n",
    "        out = self.transformer.decoder(tgt_embedded, encoder_output, tgt_mask)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 74, 13685])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = Seq2SeqTransformerConfig(vocab_size_src, vocab_size_tgt, MAX_LENGTH)\n",
    "# model = Seq2SeqTransformer(config, tokenizer_vi)\n",
    "# pred = model.forward(preprocessed_train[0: 1]['input_ids'],\n",
    "#               preprocessed_train[0: 1]['labels'])\n",
    "# pred[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Seq2SeqTransformerConfig(vocab_size_src, vocab_size_tgt, MAX_LENGTH)\n",
    "model = Seq2SeqTransformer(config, tokenizer_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/en-vi-machine-translation\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_train,\n",
    "    eval_dataset=preprocessed_val\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu==2.5.1\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu==2.5.1)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu==2.5.1)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (1.26.3)\n",
      "Requirement already satisfied: colorama in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from sacrebleu==2.5.1) (0.4.6)\n",
      "Collecting lxml (from sacrebleu==2.5.1)\n",
      "  Downloading lxml-5.3.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\asus\\ungdung\\miniconda\\workspace\\envs\\aioex\\lib\\site-packages (from portalocker->sacrebleu==2.5.1) (307)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.3.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.3.1 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install sacrebleu==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22464\\2018366447.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src = torch.tensor(src).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: When I was little , I thought my country was the best on the planet , and I grew up singing a song called & quot ; Nothing To <unk> . & quot ; <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Target: <bos> Khi tôi còn nhỏ , Tôi nghĩ rằng <unk> Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài & quot ; Chúng ta chẳng có gì phải ghen tị . & quot ; <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Predict: Heatherwick Free ăn Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng xứng ogami tấ TQ ngập Quỷ tiêng\n",
      "BLEU Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "def greedy_decode(model, src, max_len, tokenizer, device=\"cpu\"):\n",
    "    src = torch.tensor(src).unsqueeze(0).to(device)\n",
    "    mask_encoder = torch.zeros(\n",
    "        (src.size()[1], src.size()[1]), device=device).type(torch.bool)\n",
    "    context = model.encoder(src, mask_encoder)\n",
    "    y_start = torch.full((1, 1), tokenizer.added_tokens_encoder[\"<bos>\"], dtype=torch.long).to(device)\n",
    "\n",
    "    output = []\n",
    "    # teacher forcing\n",
    "    for i in range(max_len):\n",
    "        mask_decoder = generate_square_subsequent_mask(y_start.shape[1], device)\n",
    "        output_decoder = model.decoder(y_start, context, mask_decoder)\n",
    "        _, next_word = output_decoder.max(dim=-1)\n",
    "        y_start = next_word\n",
    "        output.append(next_word.item())\n",
    "\n",
    "        if next_word.item() == tokenizer.added_tokens_encoder[\"<eos>\"]:\n",
    "            break\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "def translate():\n",
    "    model.eval()\n",
    "    src = preprocessed_test[0][\"input_ids\"]\n",
    "    tgt = preprocessed_test[0][\"labels\"]\n",
    "    output = greedy_decode(model, src, MAX_LENGTH,\n",
    "                           tokenizer_vi, device=\"cpu\")\n",
    "    \n",
    "    print(\"Input:\", tokenizer_en.decode(src))\n",
    "    print(\"Target:\", tokenizer_vi.decode(tgt))\n",
    "    print(\"Predict:\", tokenizer_vi.decode(output))\n",
    "\n",
    "    bleu_score = sacrebleu.corpus_bleu(\n",
    "        [tokenizer_vi.decode(output)], [[tokenizer_vi.decode(tgt)]], force=True).score\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "translate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
