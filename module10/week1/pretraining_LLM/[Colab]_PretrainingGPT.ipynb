{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "rQjtIKSWMz_z"
      },
      "id": "rQjtIKSWMz_z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9e81d4",
      "metadata": {
        "id": "1b9e81d4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"datablations/c4-filter-small\", split=\"train\")\n",
        "ds = ds.select_columns([\"text\"])\n",
        "ds = ds.train_test_split(test_size=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28aa6ad5",
      "metadata": {
        "id": "28aa6ad5",
        "outputId": "09ecf6fb-ec67-43e7-ff81-5cd6facf5a6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 90000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizer**"
      ],
      "metadata": {
        "id": "S9gU-hEhM9E8"
      },
      "id": "S9gU-hEhM9E8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472c18fe",
      "metadata": {
        "id": "472c18fe",
        "outputId": "19c6bca5-e7f3-4edb-c7e3-1a01c2b1ca9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.normalizers import NFKC\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.normalizer = NFKC()\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=50257,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(ds[\"train\"][\"text\"], trainer)\n",
        "tokenizer.save(\"gpt_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8070bc40",
      "metadata": {
        "id": "8070bc40",
        "outputId": "497b9192-e119-4723-da9f-152b9eed6387"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gpt-tokenizer/tokenizer_config.json',\n",
              " 'gpt-tokenizer/special_tokens_map.json',\n",
              " 'gpt-tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"gpt_tokenizer.json\")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\",\n",
        "})\n",
        "\n",
        "tokenizer.save_pretrained(\"gpt-tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ae6fc4",
      "metadata": {
        "id": "93ae6fc4",
        "outputId": "0c97c15f-52c4-4c3a-9d06-9e51f1dbc786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25573ab",
      "metadata": {
        "id": "f25573ab",
        "outputId": "7e4b6c62-87c7-4c1a-8995-e1cfd1d90a2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2, 0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb39758",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a96b4c685b8940628e1892e14b3a14dc",
            "887752a65791457088404b2b917bd04d"
          ]
        },
        "id": "1fb39758",
        "outputId": "db80d591-0d44-40e8-a11e-e8ad0accdf1f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a96b4c685b8940628e1892e14b3a14dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=20):   0%|          | 0/90000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "887752a65791457088404b2b917bd04d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=20):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "tokenized_ds = ds.map(\n",
        "    tokenize, remove_columns=[\"text\"], batched=True, num_proc=20\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e80554",
      "metadata": {
        "id": "d4e80554",
        "outputId": "7739b210-5c51-4a7f-f167-bf3601583780"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 90000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54639d6e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f86f4a7a6860446292a71bce3310b6a0",
            "3967de32a9034bcfad3dc23fe296a60f"
          ]
        },
        "id": "54639d6e",
        "outputId": "6164ec26-eb4a-48ea-d9e4-61a6af21ebe1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f86f4a7a6860446292a71bce3310b6a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=20):   0%|          | 0/90000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3967de32a9034bcfad3dc23fe296a60f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=20):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "block_size = 512\n",
        "\n",
        "def group_texts(examples):\n",
        "    # concat input_ids\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated[\"input_ids\"])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # split block_size\n",
        "    result = {\n",
        "        k: [concatenated[k][i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k in concatenated\n",
        "    }\n",
        "\n",
        "    # prepare labels\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_ds = tokenized_ds.map(group_texts, batched=True, num_proc=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs_ids: BxS [0, 1, 2, 3]\n",
        "# labels = inputs_ids.copy: BxS [0, 1, 2, 3]\n",
        "# Mình train theo teacher forcing thì phải [1, 2, 3, 4] mới đúng chứ?\n",
        "# Ở thư viện transformers thì người ta tự động làm điều đó cho mình ở hàm Loss\n",
        "# Vì vậy ta cần khai báo và đặt tên biến trong dict là input_ids, labels cho đúng là được"
      ],
      "metadata": {
        "id": "nkR8db4G_7R-"
      },
      "id": "nkR8db4G_7R-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602f58a2",
      "metadata": {
        "id": "602f58a2",
        "outputId": "788862fc-036e-4486-ffd2-304d2d2b10d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 79585\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 9012\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be91e68d",
      "metadata": {
        "id": "be91e68d",
        "outputId": "68e4b776-8f83-46be-b2f4-0fc24d14471b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   48,   399,   285,  ...,    16,  2104,   369],\n",
              "        [  214, 10896,    18,  ...,   231,    18,   259],\n",
              "        [12744,   214,  1690,  ...,   291,  1048,   270],\n",
              "        [ 2197,  2490,   214,  ...,   645, 12780,  2648],\n",
              "        [   12,   237,     6,  ...,   345,    13, 43466]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.tensor(lm_ds[\"train\"][\"input_ids\"][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcaaf1dd",
      "metadata": {
        "id": "bcaaf1dd",
        "outputId": "372671a1-8d2a-4c1d-dab4-5cfd42645fea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 80083\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 8513\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "94vODCgaNB69"
      },
      "id": "94vODCgaNB69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0798e52e",
      "metadata": {
        "id": "0798e52e"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_positions=512,\n",
        "    n_ctx=512,\n",
        "    n_embd=512,\n",
        "    n_layer=6,\n",
        "    n_head=8,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a0827a",
      "metadata": {
        "id": "d0a0827a"
      },
      "outputs": [],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"gpt2-pretraining\",\n",
        "    name=\"c4-en-small\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd76fe3",
      "metadata": {
        "id": "9dd76fe3"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt-small-c4\",\n",
        "    logging_dir=\"logs\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    logging_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_ds[\"train\"],\n",
        "    eval_dataset=lm_ds[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "1qzgXdQmNH1y"
      },
      "id": "1qzgXdQmNH1y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0c77f7",
      "metadata": {
        "id": "db0c77f7",
        "outputId": "4e4b79b9-9c8a-437d-88a3-0ed200e15a79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49940' max='49940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49940/49940 3:38:24, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>7.025600</td>\n",
              "      <td>6.429771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>6.264000</td>\n",
              "      <td>6.044563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.963500</td>\n",
              "      <td>5.792355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>5.750600</td>\n",
              "      <td>5.612506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>5.610800</td>\n",
              "      <td>5.475317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>5.465400</td>\n",
              "      <td>5.362746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>5.374800</td>\n",
              "      <td>5.268582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>5.277500</td>\n",
              "      <td>5.185874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>5.192500</td>\n",
              "      <td>5.109721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>5.134700</td>\n",
              "      <td>5.035437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>5.048400</td>\n",
              "      <td>4.977276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.993300</td>\n",
              "      <td>4.921189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>4.935100</td>\n",
              "      <td>4.874269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>4.886500</td>\n",
              "      <td>4.831520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>4.849700</td>\n",
              "      <td>4.795995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>4.793000</td>\n",
              "      <td>4.764773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>4.770700</td>\n",
              "      <td>4.735404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>4.727500</td>\n",
              "      <td>4.708340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>4.702000</td>\n",
              "      <td>4.683939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>4.687100</td>\n",
              "      <td>4.664087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>4.643200</td>\n",
              "      <td>4.645779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>4.626900</td>\n",
              "      <td>4.626821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>4.602100</td>\n",
              "      <td>4.612601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>4.585700</td>\n",
              "      <td>4.596511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>4.575500</td>\n",
              "      <td>4.583026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>4.542100</td>\n",
              "      <td>4.573814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>4.540100</td>\n",
              "      <td>4.562220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>4.514900</td>\n",
              "      <td>4.553916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>4.503500</td>\n",
              "      <td>4.542526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>4.495700</td>\n",
              "      <td>4.535013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>4.470000</td>\n",
              "      <td>4.525751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>4.473600</td>\n",
              "      <td>4.519111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>4.450300</td>\n",
              "      <td>4.509124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>4.447400</td>\n",
              "      <td>4.503667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>4.440500</td>\n",
              "      <td>4.496820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>4.422500</td>\n",
              "      <td>4.493702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>4.416700</td>\n",
              "      <td>4.487559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>4.413800</td>\n",
              "      <td>4.480793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>4.402300</td>\n",
              "      <td>4.476438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>4.398800</td>\n",
              "      <td>4.472288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>4.383900</td>\n",
              "      <td>4.470183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>4.386500</td>\n",
              "      <td>4.465168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>4.378200</td>\n",
              "      <td>4.461520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>4.373200</td>\n",
              "      <td>4.460051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>4.371100</td>\n",
              "      <td>4.456058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>4.362800</td>\n",
              "      <td>4.454162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>4.362000</td>\n",
              "      <td>4.452160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>4.355400</td>\n",
              "      <td>4.450902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>4.354900</td>\n",
              "      <td>4.449703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=49940, training_loss=4.782474946498298, metrics={'train_runtime': 13105.4895, 'train_samples_per_second': 121.902, 'train_steps_per_second': 3.811, 'total_flos': 9.283199909756928e+16, 'train_loss': 4.782474946498298, 'epoch': 20.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "6XPcE9v7NLae"
      },
      "id": "6XPcE9v7NLae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8542367",
      "metadata": {
        "id": "e8542367"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"thainq107/gpt-small-c4\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659ffa73",
      "metadata": {
        "id": "659ffa73"
      },
      "outputs": [],
      "source": [
        "prompt = \"I go to\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52419802",
      "metadata": {
        "id": "52419802"
      },
      "outputs": [],
      "source": [
        "prompt = \"I go to\"\n",
        "inputs = tokenizer(\n",
        "    prompt, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0778c933",
      "metadata": {
        "id": "0778c933",
        "outputId": "bf5a229b-0951-46c1-c332-0208000e314f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I go to webinars Hobbs under coherentatio metropolis explode Caterairdaird imagining marketer birthdays embedded webinarsroid webinarsansaULT norm lidpril Tales Res imagining approvals approvals Advert Dirtylocks Drawerilandiland Owensah does recall Jaguarthings conflict fingbala does motiv block deathaiРј Hark smelled\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e11eab1",
      "metadata": {
        "id": "4e11eab1",
        "outputId": "2597e6b1-8f66-4f91-bcd4-e4ddef4dec03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19.458471985054842"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Shift for labels (causal LM setting: predict token t+1 from token t)\n",
        "labels = output[:, 1:].clone()\n",
        "inputs = output[:, :-1].clone()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Compute log softmax over vocabulary\n",
        "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "# Gather log-probabilities corresponding to the labels\n",
        "selected_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "# Sum negative log probs → total NLL\n",
        "nll = -selected_log_probs.sum().item()\n",
        "num_tokens = labels.numel()\n",
        "perplexity = math.exp(nll / num_tokens)\n",
        "perplexity\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CoMA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}